<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<style>
  * { margin: 0; padding: 0; box-sizing: border-box; }
  body {
    font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
    font-size: 16px;
    line-height: 1.6;
    color: #1a1a1a;
    padding: 48px 56px;
    max-width: 800px;
    background: white;
    overflow: hidden;
  }
  h1 { font-size: 28px; margin-bottom: 8px; }
  h2 { font-size: 22px; margin-top: 40px; margin-bottom: 12px; border-bottom: 1px solid #e5e5e5; padding-bottom: 4px; }
  h3 { font-size: 18px; margin-top: 28px; margin-bottom: 8px; }
  p { margin-bottom: 16px; }
  .subtitle { color: #666; font-size: 18px; margin-bottom: 32px; }
  code { background: #f0f0f0; padding: 2px 6px; border-radius: 3px; font-size: 14px; }
  pre { background: #f5f5f5; padding: 16px; border-radius: 6px; margin-bottom: 16px; overflow-x: auto; }
  pre code { background: none; padding: 0; }
  blockquote { border-left: 3px solid #ccc; padding-left: 16px; margin: 16px 0; color: #555; }
  ul, ol { margin-bottom: 16px; padding-left: 28px; }
  li { margin-bottom: 4px; }
  .theorem { background: #f8f4ff; border-left: 3px solid #7c3aed; padding: 16px; margin: 20px 0; border-radius: 0 6px 6px 0; }
  .theorem-title { font-weight: 600; margin-bottom: 4px; }
  .figure { text-align: center; margin: 24px 0; }
  .figure img { max-width: 100%; border: 1px solid #eee; border-radius: 4px; }
  .figure-caption { font-size: 14px; color: #666; margin-top: 8px; }
  table { border-collapse: collapse; margin: 16px 0; width: 100%; }
  th, td { border: 1px solid #ddd; padding: 8px 12px; text-align: left; }
  th { background: #f5f5f5; font-weight: 600; }
</style>
<script>
  // MathJax for rendering math
  window.MathJax = {
    tex: { inlineMath: [['$', '$']], displayMath: [['$$', '$$']] },
    svg: { fontCache: 'global' }
  };
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js" async></script>
</head>
<body>

<h1>Point and Interval Estimates</h1>
<p class="subtitle">A Test Document for HTML Rendering in TLDraw</p>

<h2>1. Introduction</h2>

<p>Statistical estimation is the process of using data to infer the values of unknown population parameters. The two main types of estimates are <strong>point estimates</strong> and <strong>interval estimates</strong>.</p>

<p>A point estimate is a single value used as an estimate of a population parameter. For example, the sample mean $\bar{X} = \frac{1}{n}\sum_{i=1}^{n} X_i$ is a point estimate of the population mean $\mu$.</p>

<p>An interval estimate provides a range of values within which the parameter is expected to lie, along with a measure of confidence.</p>

<h2>2. Properties of Estimators</h2>

<h3>2.1 Bias and Variance</h3>

<p>The <strong>bias</strong> of an estimator $\hat{\theta}$ for a parameter $\theta$ is defined as:</p>

$$\text{Bias}(\hat{\theta}) = E[\hat{\theta}] - \theta$$

<p>An estimator is <strong>unbiased</strong> if $\text{Bias}(\hat{\theta}) = 0$, meaning $E[\hat{\theta}] = \theta$.</p>

<p>The <strong>variance</strong> of an estimator measures its spread around its expected value:</p>

$$\text{Var}(\hat{\theta}) = E\left[(\hat{\theta} - E[\hat{\theta}])^2\right]$$

<p>The <strong>mean squared error</strong> combines both:</p>

$$\text{MSE}(\hat{\theta}) = \text{Bias}(\hat{\theta})^2 + \text{Var}(\hat{\theta})$$

<div class="theorem">
  <div class="theorem-title">Theorem 2.1 (Cramer-Rao Lower Bound)</div>
  <p>For any unbiased estimator $\hat{\theta}$ of $\theta$, the variance satisfies:</p>
  $$\text{Var}(\hat{\theta}) \geq \frac{1}{I(\theta)}$$
  <p>where $I(\theta) = -E\left[\frac{\partial^2}{\partial\theta^2}\log f(X;\theta)\right]$ is the Fisher information.</p>
</div>

<h3>2.2 Consistency</h3>

<p>An estimator $\hat{\theta}_n$ is <strong>consistent</strong> if it converges in probability to the true parameter value as the sample size increases:</p>

$$\hat{\theta}_n \xrightarrow{P} \theta \quad \text{as } n \to \infty$$

<p>A sufficient condition for consistency is that both the bias and variance converge to zero:</p>

<ul>
  <li>$\text{Bias}(\hat{\theta}_n) \to 0$ as $n \to \infty$</li>
  <li>$\text{Var}(\hat{\theta}_n) \to 0$ as $n \to \infty$</li>
</ul>

<h2>3. Confidence Intervals</h2>

<p>A <strong>confidence interval</strong> is an interval estimate associated with a confidence level $1 - \alpha$. For a parameter $\theta$, a $(1-\alpha)$ confidence interval $(L, U)$ satisfies:</p>

$$P(L \leq \theta \leq U) = 1 - \alpha$$

<h3>3.1 Normal Distribution</h3>

<p>For a sample from a normal distribution with known variance $\sigma^2$, the $(1-\alpha)$ confidence interval for the mean is:</p>

$$\bar{X} \pm z_{\alpha/2} \cdot \frac{\sigma}{\sqrt{n}}$$

<p>where $z_{\alpha/2}$ is the upper $\alpha/2$ quantile of the standard normal distribution.</p>

<h3>3.2 Student's t-Distribution</h3>

<p>When the variance is unknown and estimated by $S^2$, we use the t-distribution. The confidence interval becomes:</p>

$$\bar{X} \pm t_{\alpha/2, n-1} \cdot \frac{S}{\sqrt{n}}$$

<p>where $t_{\alpha/2, n-1}$ is the critical value from the t-distribution with $n-1$ degrees of freedom.</p>

<table>
  <tr><th>Confidence Level</th><th>$z_{\alpha/2}$</th><th>Interpretation</th></tr>
  <tr><td>90%</td><td>1.645</td><td>Moderate confidence</td></tr>
  <tr><td>95%</td><td>1.960</td><td>Standard</td></tr>
  <tr><td>99%</td><td>2.576</td><td>High confidence</td></tr>
</table>

<h2>4. Maximum Likelihood Estimation</h2>

<p>The <strong>maximum likelihood estimator</strong> (MLE) finds the parameter value that maximizes the likelihood function:</p>

$$\hat{\theta}_{\text{MLE}} = \arg\max_\theta L(\theta; x_1, \ldots, x_n) = \arg\max_\theta \prod_{i=1}^{n} f(x_i; \theta)$$

<p>In practice, we maximize the log-likelihood:</p>

$$\ell(\theta) = \sum_{i=1}^{n} \log f(x_i; \theta)$$

<div class="theorem">
  <div class="theorem-title">Theorem 4.1 (Asymptotic Normality of MLE)</div>
  <p>Under regularity conditions, the MLE is asymptotically normal:</p>
  $$\sqrt{n}(\hat{\theta}_{\text{MLE}} - \theta_0) \xrightarrow{d} N\left(0, \frac{1}{I(\theta_0)}\right)$$
  <p>where $\theta_0$ is the true parameter value and $I(\theta_0)$ is the Fisher information.</p>
</div>

<h2>5. Bayesian Estimation</h2>

<p>In the Bayesian framework, parameters are treated as random variables with a <strong>prior distribution</strong> $\pi(\theta)$. After observing data $x$, we update to the <strong>posterior distribution</strong>:</p>

$$\pi(\theta | x) = \frac{f(x|\theta)\pi(\theta)}{\int f(x|\theta)\pi(\theta) d\theta} \propto f(x|\theta)\pi(\theta)$$

<p>Common Bayesian point estimates include:</p>

<ul>
  <li><strong>Posterior mean:</strong> $E[\theta|x] = \int \theta \cdot \pi(\theta|x) d\theta$</li>
  <li><strong>Posterior mode (MAP):</strong> $\arg\max_\theta \pi(\theta|x)$</li>
  <li><strong>Posterior median:</strong> the value $m$ such that $P(\theta \leq m|x) = 0.5$</li>
</ul>

<blockquote>
  <p>"The Bayesian approach provides a natural framework for combining prior information with observed data, yielding a complete posterior distribution rather than a single point estimate."</p>
</blockquote>

<h2>6. Comparing Estimators</h2>

<p>When comparing two estimators $\hat{\theta}_1$ and $\hat{\theta}_2$, we often use the <strong>relative efficiency</strong>:</p>

$$\text{RE}(\hat{\theta}_1, \hat{\theta}_2) = \frac{\text{Var}(\hat{\theta}_2)}{\text{Var}(\hat{\theta}_1)}$$

<p>A value greater than 1 indicates that $\hat{\theta}_1$ is more efficient. The <strong>asymptotic relative efficiency</strong> (ARE) compares estimators in the limit as $n \to \infty$.</p>

<p>For example, when sampling from a normal distribution:</p>
<ul>
  <li>The sample mean has variance $\sigma^2/n$</li>
  <li>The sample median has asymptotic variance $\pi\sigma^2/(2n)$</li>
  <li>So $\text{ARE}(\bar{X}, \text{median}) = \pi/2 \approx 1.571$</li>
</ul>

<p>This means the sample mean is about 57% more efficient than the median for normal data.</p>

</body>
</html>
